---
title: "Visualization of Linear Models"
author: "by Edneide Ramalho"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
    html_document:
      highlight: textmate
      logo: logo.png
      theme: flatly
      number_sections: yes
      toc: yes
      toc_float:
        collapsed: yes
        smooth_scroll: no
---

# The best fit line 

In regression, we use the least squares criterion to determine the best fit line.

```{r}
library(ggplot2)
library(readr)
possum <- read_csv("possum.csv")
```

```{r}
ggplot(data = possum, aes(y = total_l, x = tail_l)) +
  geom_point() +
  geom_smooth(method = "lm")
```

-   Plot without the standard error (se):

```{r}
ggplot(data = possum, aes(y = total_l, x = tail_l)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

# Exercises

## **The "best fit" line**

The simple linear regression model for a numeric response as a function of a numeric explanatory variable can be visualized on the corresponding scatterplot by a straight line. This is a "best fit" line that cuts through the data in a way that minimizes the distance between the line and the data points.

We might consider linear regression to be a specific example of a larger class of *smooth* models. The `geom_smooth()` function allows you to draw such models over a scatterplot of the data itself. This technique is known as visualizing the model *in the data space*. The `method` argument to `geom_smooth()` allows you to specify what class of smooth model you want to see. Since we are exploring linear models, we'll set this argument to the value `"lm"`.

Note that `geom_smooth()` also takes an `se` argument that controls the standard error, which we will ignore for now.

Create a scatterplot of body weight as a function of height for all individuals in the `bdims` dataset with a simple linear model plotted over the data.

```{r}
library(openintro)
library(tidyverse)
glimpse(bdims)
```

```{r}
# Scatterplot with regression line
ggplot(data = bdims, aes(x = hgt, y = wgt)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)
```

## **Uniqueness of least squares regression line**

The least squares criterion implies that the slope of the regression line is unique. In practice, the slope is computed by R. In this exercise, you will experiment with trying to find the optimal value for the regression slope for weight as a function of height in the `bdims` dataset via trial-and-error.

To help, we've built a custom function for you called `add_line()`, which takes a single argument: the proposed slope coefficient.

```{r}
add_line <- function (my_slope) {

  bdims_summary <- bdims %>%
    summarize(N = n(), r = cor(hgt, wgt),
              mean_hgt = mean(hgt), mean_wgt = mean(wgt),
              sd_hgt = sd(hgt), sd_wgt = sd(wgt)) %>%
    mutate(true_slope = r * sd_wgt / sd_hgt, 
           true_intercept = mean_wgt - true_slope * mean_hgt)
  p <- ggplot(data = bdims, aes(x = hgt, y = wgt)) + 
    geom_point() + 
    geom_point(data = bdims_summary, 
               aes(x = mean_hgt, y = mean_wgt), 
               color = "red", size = 3)
  
  my_data <- bdims_summary %>%
    mutate(my_slope = my_slope, 
           my_intercept = mean_wgt - my_slope * mean_hgt)
  p + geom_abline(data = my_data, 
                  aes(intercept = my_intercept, slope = my_slope), color = "dodgerblue")
}
```

```{r}
# Estimate optimal value of my_slope
add_line(my_slope = 1)
```

# Understanding Linear Model

**Generic statistical model:**

response = f(explanatory) + noise

**Regression model:**

$$
Y = \beta_0 + \beta_1 \times X + \epsilon, \,\,\, \epsilon \sim N(0, \sigma_\epsilon)
$$

**Fitted values:**

$$
\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 \times X 
$$

**Residuals:**

$$
e = Y - \hat{Y}
$$

**Fitting procedure:**

-   Given $n$ observations of pairs $(x_i, y_i)$ ...

-   Find $\hat{\beta}_0$ that minimizes $\sum_{i = 1}^{n}{e^{2}_i}$

**Least squares:** minimizes the squared sum of residuals.

# Regression modeling

-   Techniques for modeling a quantitative repsonse

-   Types o regression models:

    -   Least squares

    -   Weighted

    -   Generalized

    -   Nonparametric

    -   Ridge

    -   Bayesian, etc

# Exercises

## **Regression to the mean**

*Regression to the mean* is a concept attributed to Sir Francis Galton. The basic idea is that extreme random observations will tend to be less extreme upon a second trial. This is simply due to chance alone. While "regression to the mean" and "linear regression" are not the same thing, we will examine them together in this exercise.

One way to see the effects of regression to the mean is to compare the heights of parents to their children's heights. While it is true that tall mothers and fathers tend to have tall children, those children tend to be less tall than their parents, relative to average. That is, fathers who are 3 inches taller than the average father tend to have children who may be taller than average, but by less than 3 inches.

The `Galton_men` and `Galton_women` datasets contain data originally collected by Galton himself in the 1880s on the heights of men and women, respectively, along with their parents' heights.

Compare the slope of the regression line to the slope of the diagonal line. What does this tell you?

-   Create a scatterplot of the height of men as a function of their father's height. Add the simple linear regression line and a diagonal line (with slope equal to 1 and intercept equal to 0) to the plot.

-   Create a scatterplot of the height of women as a function of their mother's height. Add the simple linear regression line and a diagonal line to the plot.

    ```{r}
    library(readr)
    Galton_women <- read_csv("Galton_women.csv", 
        col_types = cols(sex = col_character()))
    Galton_men <- read_csv("Galton_men.csv")
    ```

```{r}
# Height of children vs. height of father
ggplot(data = Galton_men, aes(x = father, y = height)) +
  geom_point() + 
  geom_abline(slope = 1, intercept = 0) + 
  geom_smooth(method = "lm", se = FALSE)


```

```{r}
# Height of children vs. height of mother
ggplot(data = Galton_women, aes(x = mother, y = height)) +
  geom_point() + 
  geom_abline(slope = 1, intercept = 0) + 
  geom_smooth(method = "lm", se = FALSE)
```
